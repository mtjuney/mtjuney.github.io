---
layout: post
title:  "自己符号化器"
date:   2015-12-01 02:00:00 +0900
categories: DeepLearning
---
Deep Learning勉強会 第4回


# 5.1 概要

自己符号化器はニューラルネットにおいて，目標とする出力(正解データ)を入力と同じにした教師なし学習を行うときに用いられるモデル．

自己符号化器の中間層において，データが一度別の表現に置き換わる事を利用してデータの特徴抽出，ディープニューラルネットの事前学習などに用いられる．

一般的な自己符号化器では，図のような単層のニューラルネット(入力層と出力層のみからなるニューラルネット)を折り返したものを使用する．

このとき入力層から中間層への計算は，重み行列を\\(\mathbf{W}\\)，バイアスを\\(\mathbf{b}\\)，活性化関数を\\(f\\)とし，入力層に\\(\mathbf{x}\\)が入力された時の中間層のベクトルを\\(\mathbf{y}(\mathbf{x})\\)とすると

\\[ \mathbf{y}(\mathbf{x}) = f(\mathbf{W}\mathbf{x} + \mathbf{b}) \\]

となる．

さらに，出力を\\(\hat{\mathbf{x}}(\mathbf{x})\\)とすると，中間層から出力層への計算は

\\[ \hat{\mathbf{x}}(\mathbf{x}) = \tilde{f}(\tilde{\mathbf{W}}\mathbf{y}(\mathbf{x}) + \tilde{\mathbf{b}}) \\]  

と表せる．

まとめると，\\(\mathbf{x}\\)が入力された時の自己符号化器の出力\\(\hat{\mathbf{x}}(\mathbf{x})\\)は  

\\[
 \hat{\mathbf{x}}(\mathbf{x}) = \tilde{f}(\tilde{\mathbf{W}}  f(\mathbf{W}\mathbf{x} + \mathbf{b} )  + \tilde{\mathbf{b}})
\\]  

となる．


このようなモデルに対して，訓練データ\\(\mathbf{x}\_1 , \mathbf{x}\_2 , \mathbf{x}\_3 ,..., \mathbf{x}\_N \\) が与えられた時，任意のサンプル \\(\mathbf{x}\_n\\) を入力した時の出力 \\(\hat{\mathbf{x}}(\mathbf{x}\_n)\\) が \\(\mathbf{x}\_n\\)に近付くように重みを調整していく．

このとき，

\\[
 \mathbf{y}(\mathbf{x}) = f(\mathbf{W}\mathbf{x} + \mathbf{b})
\\]

を符号化，

\\[ \displaystyle
 \hat{\mathbf{x}}(\mathbf{x}) = \tilde{f}(\tilde{\mathbf{W}}\mathbf{y}(\mathbf{x}) + \tilde{\mathbf{b}})
\\]

を復号化としてみなせば，\\(\mathbf{y}(\mathbf{x})\\)は\\(\mathbf{x}\\)を符号化して得られる符号として見ることがでる．  
つまり自己符号化器の出力\\(\hat{\mathbf{x}}(\mathbf{x})\\)が入力\\(\mathbf{x}\\)により近いモデルならば，それだけ中間層の\\(\mathbf{y}(\mathbf{x})\\)は\\(\mathbf{x}\\)の特徴をよりうまく抽出した符号だといえる．


# 5.2 ネットワークの設計

単層ニューラルネットから作った自己符号化器には，中間層の\\(f\\)と出力層の\\(\tilde{f}\\)の二つの活性化関数がある．中間層の\\(f\\)は自由に決めてよいが，出力層の\\(\tilde{f}\\)は与える入力を表現できるものを選ぶ(例えば範囲制限のない実数なら恒等写像など)．

最小化する誤差関数は他のニューラルネットでよく使用されているものでよい(出力層が恒等写像なら二乗誤差の和，ロジスティック関数なら交差エントロピーなど)．

#### 重み共有

入力層のユニット数が\\(D\_x\\)，中間層のユニット数が\\(D\_y\\)のとき，重み行列のサイズはそれぞれ\\(\mathbf{W}\\)は\\(D\_x \times D\_y\\)，\\(\tilde{\mathbf{W}}\\)は\\(D\_y \times D\_x\\)．  
学習を行うとき，この二つの重みを

\\[
 \tilde{\mathbf{W}} = \mathbf{W}^T
\\]

の形で共通にすることでネットワークの自由度を制約することがある．これで学習時の過適合を防げる場合もあるが，どのようなときに重み共有にすればよいかといった知見はない．  
また重み共有をせずとも，学習後のモデルで重みがこの関係に近い値になることも．


# 5.3 自己符号化器の働き

自己符号化器では，中間層のユニット数を入力層のユニット数より少ない数にすると入力データのベクトルをより少ない次元数で表現することになるため，次元削減のような効果を得ることができる．

ただし\\(\mathbf{b}\\)と\\(\tilde{\mathbf{b}}\\)がともに\\(\mathbf{0}\\)，\\(f\\)と\\(\tilde{f}\\)がともに恒等写像であるとした場合，上の式が

\\[
 \hat{\mathbf{x}}(\mathbf{x}) = \tilde{\mathbf{W}} \mathbf{W} \mathbf{x}
\\]

となる．  
ここで中間層のユニット数が入力層のユニット数と同じかそれより多い場合，\\(\mathbf{W}\\)と\\(\tilde{\mathbf{W}}\\)のランクの性質から，

\\[
 \tilde{\mathbf{W}} \mathbf{W} = \mathbf{I}
\\]

となる可能性があり，そうなると\\(\mathbf{x}\\)が単に恒等写像されただけになり全く意味のないモデルになってしまう．


中間層のユニット数を入力層のユニット数より少なくすると学習によって得られる重み\\(\mathbf{W}\\)は，主成分分析によって得られるものとほぼ同等のものになる(証明は略)．


# 5.4 スパース正則化

学習を行うとき，中間層の中でなるべく少ないユニットのみを活性化させる制約を加えるスパース正則化を行うことで，中間層のユニット数が入力層のユニット数よりも多いような状況でもうまく入力の特徴を表現する重みを得ることができ，これを **過完備な(overcomplete)** 表現という．

具体的にはもともとの誤差関数\\(E(\mathbf{w})\\)に正則化項を加えたもの，

<!-- \\[
 \tilde{E}(\mathbf{w}) \equiv E(\mathbf{w}) + \beta \sum\_{j=1}^{D\_y} \mathrm{KL}(\rho \| \hat{\rho}\_j)
\\] -->

\\[
 \tilde{E}(\mathbf{w}) \equiv E(\mathbf{w}) + \beta \sum\_{j=1}^{D} \mathrm{KL}(\rho \| \hat{\rho})
\\]

を最小化する．  
\\(\hat{\rho}\_j\\)はj番目のユニットの平均活性度の推定値，\\(\rho\\)はその目標値，\\(\mathrm{KL}(\rho \| \hat{\rho}\_j)\\)はカルバック・ライブラー・ダイバージェンスと言われこの二つの近さを与えるもので，

\\[
 \mathrm{KL}( \rho \| \hat{\rho}\_j ) = \rho \log \left( \frac{\rho}{\hat{\rho}\_j} \right) + (1 - \rho) \log \left( \frac{1 - \rho}{1 - \hat{\rho}\_j} \right)
\\]

で計算される．  
この正則化は中間層の活性化関数が非負の値を返す場合に有効である．

#### 最適化について

正則化項を加えた\\(\tilde{E}(\mathbf{w})\\)を勾配降下法によって最小化することを考える．

3章で紹介された重み減衰も誤差関数に新たな項を付け足したものを最小化していたが，あちらは重みに対する制約なので，重みの更新式(つまり\\(\partial \mathbf{W}\\)を求める式)を修正するだけでよかった．  
対してスパース正規化は対象とした層のユニットの活性度に対する制約である．ユニットの活性度はその層よりも入力側にあるパラメータの影響を受けるため，正則化項の影響を入力側に伝搬させなければならない．そのためにはデルタの計算式を修正する必要がある．

第\\(l\\)層のデルタを求めるためには\\(\frac{\partial \tilde{E}(\mathbf{w})}{\partial u\_j^{(l)}}\\)を計算する必要があるが，微分は線形変換なので追加した正規化項の微分を求めてそれをデルタに足せばよい．  
したがって，

\\[
  \frac{\partial}{\partial u\_j^{(l)}} \left( \beta \sum\_{j=1}^{D\_l} \mathrm{KL}( \rho \| \hat{\rho}\_j ) \right)
\\]

の値を求めればよく，計算すると，最終的に第\\(l\\)層のデルタを求める式は

\\[
  \delta\_l^{(l)} = \left( \sum\_k \delta\_k^{(l+1)} w\_{kj}^{(l+1)} + \beta \left( - \frac{\rho}{\hat{\rho}\_j} + \frac{1 - \rho}{1 - \hat{\rho}\_j} \right) \right) f'(u\_j^{(l)})
\\]

となる．
正規化項は中間層の活性化の制約のため，出力層でのデルタには影響しない．

計算に必要な平均活性度\\(\hat{\rho}\_j\\)は，実際には全サンプルでの平均活性度を求めるのは非効率なのでその時のミニバッチの平均活性度を求め，過去の値との加重平均をとったものをその時の修正に使う平均活性度\\(\hat{\rho}\_j\\)としている．

\\[
\hat{\rho}\_j^{(t)} = \lambda \hat{\rho}\_j^{(t-1)} + (1 - \lambda) \hat{\rho}\_j
\\]

\\(\lambda\\)は重みである．


# 5.5 データの白色化

データの偏りをなくすために正規化や白色化が用いられる．ここでは白色化を扱う．

白色化の目的はデータの成分間の相関を無くすことである．

#### 白色化の手順

\\(D\\)次元の訓練用サンプル\\(\mathbf{x} = [ x\_1 ,...,x\_D ]\\)に対し，  
各サンプルから全サンプルの平均を引いたものを\\(\mathbf{x}\_1 ,..., \mathbf{x}\_N\\)とする．  
サンプルの成分間の相関は共分散行列

\\[
\Phi\_X \equiv \frac{1}{N} \sum\_{n=1}^N \mathbf{x}\_n \mathbf{x}\_n^T = \frac{1}{N} \mathbf{X} \mathbf{X}^T
\\]

によって表される．  
この共分散行列の非対角成分が0であれば成分間の相関は無いことになる．

ここで，任意のサンプル\\(\mathbf{x}\_n\\)にある\\(D \times D\\)行列\\(\mathbf{P}\\)による線形変換を行って\\(\mathbf{u}\_n\\)を得るとする．

\\[
\mathbf{u}\_n = \mathbf{P} \mathbf{x}\_n
\\]

よって全ての\\(\mathbf{u}\_n\\)に対する共分散行列が

\\[
\Phi\_U \equiv \frac{1}{N} \sum\_{n=1}^N \mathbf{u}\_n \mathbf{u}\_n^T = \frac{1}{N} \mathbf{U} \mathbf{U}^T = \frac{1}{N} \mathbf{P} \mathbf{X} \mathbf{X}^T \mathbf{P}^T
\\]

となる．

ここで共分散行列\\(\Phi\_U\\)が対角行列になるように\\(\mathbf{P}\\)を選べば，元のサンプル\\(\mathbf{x}\_1 ,..., \mathbf{x}\_N\\)に\\(\mathbf{P}\\)を掛けることで白色化を行うことができる．  
目標とする対角行列を単位行列\\(\mathbf{I}\\)とすると，\\(\mathbf{P}\\)が満たすべき式が以下のように導出できる．

\\[
\begin{aligned}
\Phi\_U & = \mathbf{I} \\
\frac{1}{N} \mathbf{P} \mathbf{X} \mathbf{X}^T \mathbf{P}^T & = \mathbf{I} \\
\frac{1}{N} \mathbf{X} \mathbf{X}^T & = \left( \mathbf{P} \right)^{-1} \left( \mathbf{P}^T \right)^{-1} \\
\Phi\_X & = \left( \mathbf{P}^T \mathbf{P} \right)^{-1} \\
\\
\mathbf{P}^T \mathbf{P} & = \Phi\_X^{-1}
\end{aligned}
\\]

\\(\Big(\\) 行列積の転置と逆行列に注意 : \\(( \mathbf{A} \mathbf{B} )^T = \mathbf{B}^T \mathbf{A}^T\\) 及び \\(( \mathbf{A} \mathbf{B} )^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1} \\) \\(\Big)\\)

また，固有ベクトルの定義から\\(\Phi\_X\\)は

\\[
\Phi\_X = \mathbf{E} \mathbf{D} \mathbf{E}^T
\\]

と表せる．ここで，

* \\(\mathbf{D}\\)は\\(\Phi\_X\\)の固有値\\(\lambda\_1 ,..., \lambda\_D\\)を対角成分に持つ対角行列
* \\(\mathbf{E} = [ \mathbf{e}\_1 ,..., \mathbf{e}\_D ]\\)．　\\(\mathbf{e}\_1 ,..., \mathbf{e}\_D\\)は\\(\Phi\_X\\)の固有値\\(\lambda\_1 ,..., \lambda\_D\\)に対応する固有ベクトル

である．

\\(\mathbf{E}\\)が直交行列であることから，

\\[
\mathbf{P} = \mathbf{Q} \mathbf{D}^{-1/2} \mathbf{E}^T
\\]

と表せる．ここで，

* \\(\mathbf{Q}\\)は\\(\mathbf{P}\\)と同じサイズの任意の直交行列
* \\(\mathbf{D}^{-1/2}\\)は\\(\mathbf{D}\\)の対角成分を\\(-1/2\\)乗したもの

であることに注意する．

\\(\mathbf{Q}\\)は\\(\mathbf{P}\\)と同サイズの直交行列であれば自由に決めて良いので，\\(\mathbf{Q} = \mathbf{I}\\)とすれば，

\\[
  \mathbf{P} = \mathbf{D}^{-1/2} \mathbf{E}^T
\\]

となる．

この白色化は共分散行列の固有値を利用するという点がPCA(主成分分析)と似ているので，ここで求めた\\(\mathbf{P}\\)を **PCA白色化** と呼ぶことにする(一般に呼ばれてはいない？)．

また，\\(\mathbf{P}\\)を対称行列に制限したものもあり，それは **ゼロ位相白色化(ZCA)** と呼ばれる(こっちは呼ばれている)．  
ZCAに用いる\\(\mathbf{P}\_{ZCA}\\)は上式を\\(\mathbf{Q} = \mathbf{E}\\)とすればよく，

\\[
\mathbf{P}\_{ZCA} = \mathbf{E} \mathbf{D}^{-1/2} \mathbf{E}^T
\\]

となる．

どちらも実際に計算する際，サンプルによっては\\(\mathbf{D}\\)の対角成分のどこかが0(どこかの成分の分散が0)になることもあり，そうなると\\(\mathbf{D}^{-1/2}\\)が計算できなくなってしまうので，小さな値\\(\epsilon(= 10^{-6} \textrm{など})\\)を加算して

\\[
\mathbf{P} \equiv \mathbf{Q} ( \mathbf{D} + \epsilon \mathbf{I})^{-1/2} \mathbf{E}^T
\\]

とする．

# 5.6 ディープネットの事前学習

通常ニューラルネットでは学習を行う前の重みはランダムな値で初期化するが，予め自己符号化器で学習させた重みを用いることでよりうまく学習させることができる(精度が良くなる and/or 収束が早くなる？)．

#### 事前学習の手順

次のような3層のニューラルネットを事前学習するとする．

![nn]({{site.url}}/images/nn_pretrain1.svg)

このニューラルネットから出力層を取り除き，次の３つの単層ニューラルネットに分割する．

![nn]({{site.url}}/images/nn_pretrain2.svg)

ここで，\\(\mathbf{W^{(1)}}\\)から順番に学習していく．  
まず一番左の単層ニューラルネットを第1層で折り返し，自己符号化器を作って\\(\mathbf{W^{(1)}}\\)の学習を行う．  
ここで学習に使う入力データを\\(\mathbf{X\_n}\\)とする．

![nn]({{site.url}}/images/nn_pretrain3.svg)

また，この自己符号化器の中間層(第1層)のユニットの出力を\\(\mathbf{Z^{(1)}}\\)としておく(これは次の層の学習に使われる)．

![nn]({{site.url}}/images/nn_pretrain4.svg)

次に，同じように真ん中の単層ニューラルネットについても第2層で折り返すことで自己符号化器を作り，\\(\mathbf{W}^{(2)}\\)の学習を行う．  
ただし，この自己符号化器への入力データは\\(\mathbf{Z^{(1)}}\\)(最初に作った自己符号化器の中間層の出力)である．

![nn]({{site.url}}/images/nn_pretrain5.svg)

ここでは\\(\mathbf{W^{(1)}}\\)の学習は行わないことに注意する．  
同様に右側の単層ニューラルネットでも自己符号化器を創り，\\(\mathbf{W}^{(3)}\\)を学習する．

![nn]({{site.url}}/images/nn_pretrain6.svg)

最後に学習した重み\\(\mathbf{W}^{(1)}, \mathbf{W}^{(2)}, \mathbf{W}^{(3)}\\)を元のニューラルネットにセットし，出力層を付け加える(出力層の重み\\(\mathbf{W}^{(4)}\\)はランダムに初期化する)．

このように単層の自己符号化器を積み重ねたものを **積層自己符号化器** と呼ぶ．


# 5.7 その他の自己符号化器

#### 多層自己符号化器

ディープネットの事前学習を行う際，単層に分割して層ごとに自己符号化器を複数作るのではなく，ディープネットの出力のひとつ前で折り返して自己符号化器を作ることもできる．

しかし単純に考えて元のディープネットの2倍近くの深さになり，学習が困難になることが多い．
